import pyspark

sc = pyspark.SparkContext()

#z = 3 and set_size = 10 lines
#result = [('u3,m2', (0.2000635487416773, 1)), ('u3,m3', (0.19999121136807602, 0)), ('u3,m1', (0.19985689670547072, 0)), ('u3,m4', (0.19946045842556148, 0)), ('u3,m5', (0.20062788475921445, 1)), ('u1,m1', (0.20034233337041701, 1)), ('u1,m2', (0.20000795705883212, 0)), ('u1,m3', (0.1999053432808634, 1)), ('u4,m3', (0.19996959828840566, 0)), ('u5,m4', (0.2003215033827588, 1)), ('u1,m4', (0.19964706878928215, 0)), ('u5,m5', (0.1997405893964275, 0)), ('u1,m5', (0.20009729750060526, 0)), ('u4,m2', (0.1999556941744704, 0)), ('u4,m1', (0.2002404312359597, 1)), ('u2,m1', (0.1996346520098503, 0)), ('u5,m1', (0.19992568667830218, 0)), ('u2,m3', (0.20009606341602582, 0)), ('u5,m2', (0.19997443689588243, 1)), ('u4,m5', (0.19956953544347772, 1)), ('u4,m4', (0.20026474085768645, 0)), ('u2,m2', (0.19999836312913777, 0)), ('u5,m3', (0.200037783646629, 0)), ('u2,m5', (0.19996469290027502, 1)), ('u2,m4', (0.20030622854471108, 1))]
result = [(u'u3,m2', (0.19773516124222873, 1)), (u'u3,m3', (0.19884380312692243, 0)), (u'u3,m1', (0.20081506879983008, 0)), (u'u3,m4', (0.2014055628489373, 0)), (u'u3,m5', (0.20120040398208122, 1)), (u'u1,m1', (0.20179587906608115, 1)), (u'u1,m2', (0.1960425930815746, 0)), (u'u1,m3', (0.19786608320480664, 1)), (u'u4,m3', (0.19884057061096821, 0)), (u'u5,m4', (0.19623418247489557, 1)), (u'u1,m4', (0.20243973954595168, 0)), (u'u5,m5', (0.19760356663200604, 0)), (u'u1,m5', (0.20185570510158576, 0)), (u'u4,m2', (0.19836234712718484, 0)), (u'u4,m1', (0.20164691073740879, 1)), (u'u2,m1', (0.19929650842142954, 0)), (u'u5,m1', (0.19644563297525022, 0)), (u'u2,m3', (0.20089501266149923, 0)), (u'u5,m2', (0.20616208752204465, 1)), (u'u4,m5', (0.20018003130871206, 1)), (u'u4,m4', (0.20097014021572596, 0)), (u'u2,m2', (0.201697811026967, 0)), (u'u5,m3', (0.20355453039580346, 0)), (u'u2,m5', (0.1991602929756148, 1)), (u'u2,m4', (0.19895037491448936, 1))]

psu = sc.parallelize(result)

ones = psu.filter(lambda x: x[1][1] == 1 )
zeroes = psu.filter(lambda x: x[1][1] == 0 )

ones_count = ones.count()
zeroes_count = zeroes.count()

ones_mean = ones.map(lambda x: x[1][0]).reduce(lambda x,y: x+y) / ones_count
zeroes_mean = zeroes.map(lambda x: x[1][0]).reduce(lambda x,y: x+y) / zeroes_count

# expected : ones_mean > zeroes_mean 
print 'mean of observed tuples:'
print ones_mean
print 'mean of unobserved tuples:'
print zeroes_mean
